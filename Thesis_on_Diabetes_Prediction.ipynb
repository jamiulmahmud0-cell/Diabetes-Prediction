{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preprocessing\n",
        "\n",
        "We begin by loading the diabetes.csv dataset and inspecting it. In this diabetes dataset, certain features use 0 to indicate missing/invalid values (e.g. BloodPressure, SkinThickness, Insulin, BMI). We replace these zeros with NaN and then apply KNN imputation (scikit-learn’s KNNImputer) to fill missing entries based on nearest neighbors. After imputation, we apply feature scaling with StandardScaler to standardize all features (zero mean, unit variance). The result is a cleaned, numeric dataset ready for modeling.\n",
        "\n",
        "Handle zeros as missing: Replace 0s with np.nan in relevant columns (except the target).\n",
        "\n",
        "Impute missing values: Use KNNImputer(n_neighbors=5) to fill in missing entries.\n",
        "\n",
        "Scale features: Fit StandardScaler on the imputed data to standardize each feature"
      ],
      "metadata": {
        "id": "hpiAGiD6yaj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/diabetes.csv')\n",
        "print(\"Original data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Identify features where 0 is invalid (treat as missing)\n",
        "cols_with_invalid_zero = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
        "df[cols_with_invalid_zero] = df[cols_with_invalid_zero].replace(0, np.nan)\n",
        "\n",
        "# Impute missing values using k-NN\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(df[cols_with_invalid_zero])\n",
        "\n",
        "# Reconstruct dataframe with imputed values\n",
        "df_imputed = df.copy()\n",
        "df_imputed[cols_with_invalid_zero] = X_imputed\n",
        "print(\"Missing values after imputation:\\n\", df_imputed.isna().sum())\n",
        "\n",
        "# Separate features and target\n",
        "feature_columns = ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n",
        "                   'Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
        "X = df_imputed[feature_columns].values\n",
        "y = df_imputed['Outcome'].values\n",
        "\n",
        "# Scale features to zero mean and unit variance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Feature means after scaling (should be ~0):\", np.round(X_scaled.mean(axis=0),3))\n",
        "print(\"Feature std devs after scaling (should be 1):\", np.round(X_scaled.std(axis=0),3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KE5ZUu3yb-_",
        "outputId": "bf7eadfa-ed5e-43e8-aa2f-a03f918eedd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape: (768, 9)\n",
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                     0.627   50        1  \n",
            "1                     0.351   31        0  \n",
            "2                     0.672   32        1  \n",
            "3                     0.167   21        0  \n",
            "4                     2.288   33        1  \n",
            "Missing values after imputation:\n",
            " Pregnancies                 0\n",
            "Glucose                     0\n",
            "BloodPressure               0\n",
            "SkinThickness               0\n",
            "Insulin                     0\n",
            "BMI                         0\n",
            "DiabetesPedigreeFunction    0\n",
            "Age                         0\n",
            "Outcome                     0\n",
            "dtype: int64\n",
            "Feature means after scaling (should be ~0): [-0. -0. -0. -0. -0.  0.  0.  0.]\n",
            "Feature std devs after scaling (should be 1): [1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Baseline Models (Decision Tree & SVM)\n",
        "\n",
        "Next, we implement two baseline classifiers: a Decision Tree and a Support Vector Machine (SVM). We split the data into training and test sets (e.g. 70% train, 30% test as commonly done) and then train each model. Finally, we evaluate performance on the test set using accuracy, precision, recall, F1-score, and the confusion matrix."
      ],
      "metadata": {
        "id": "3IygiKwxyjvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Split data: 70% train, 30% test (stratify if class imbalance is a concern)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Decision Tree classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Classification Report:\\n\",\n",
        "      classification_report(y_test, y_pred_dt, digits=4))\n",
        "print(\"Decision Tree Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n",
        "\n",
        "# Support Vector Machine (with RBF kernel)\n",
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "print(\"SVM Classification Report:\\n\",\n",
        "      classification_report(y_test, y_pred_svm, digits=4))\n",
        "print(\"SVM Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah1IhPtZyoKC",
        "outputId": "be3c7354-0fa6-44e6-94be-45904246f0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7692    0.8000    0.7843       150\n",
            "           1     0.6000    0.5556    0.5769        81\n",
            "\n",
            "    accuracy                         0.7143       231\n",
            "   macro avg     0.6846    0.6778    0.6806       231\n",
            "weighted avg     0.7099    0.7143    0.7116       231\n",
            "\n",
            "Decision Tree Confusion Matrix:\n",
            " [[120  30]\n",
            " [ 36  45]]\n",
            "SVM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7588    0.8600    0.8063       150\n",
            "           1     0.6557    0.4938    0.5634        81\n",
            "\n",
            "    accuracy                         0.7316       231\n",
            "   macro avg     0.7073    0.6769    0.6848       231\n",
            "weighted avg     0.7227    0.7316    0.7211       231\n",
            "\n",
            "SVM Confusion Matrix:\n",
            " [[129  21]\n",
            " [ 41  40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this setup, train_test_split divides the data (a 70:30 split is used here by setting test_size=0.3, following an example split. The Decision Tree (DecisionTreeClassifier) and SVM (sklearn.svm.SVC) are trained on the training set. We then use classification_report and confusion_matrix from scikit-learn to compute metrics. The classification report provides accuracy, precision, recall, and F1-score for each class, and the confusion matrix shows true/false positives and negatives.\n",
        "\n",
        "(Optionally, one could also use cross-validation to average these metrics over folds. For example, cross_val_score or cross_validate from scikit-learn can be used to evaluate accuracy or other scores with CV.)"
      ],
      "metadata": {
        "id": "ahfcmo95ysXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. LSTM Model Skeleton (PyTorch)\n",
        "\n",
        "We outline a PyTorch LSTM model to process this tabular data. Although LSTMs are typically used for sequences, here we treat each sample’s features as a sequence of length N_features. Concretely, we reshape our data so that each sample is of shape (seq_len, input_size) = (n_features, 1). The LSTM processes these “feature timesteps” and outputs a hidden representation, which we feed through a linear layer for binary classification. We include a training loop skeleton with BCEWithLogitsLoss (which combines a Sigmoid layer and binary cross-entropy)"
      ],
      "metadata": {
        "id": "3mneamgnyxya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Train/validation split (80/20)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Define LSTM-based classifier\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out  # raw logits\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "input_size = 1\n",
        "seq_len = X_scaled.shape[1]\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "model = LSTMClassifier(input_size, hidden_size, num_layers, dropout)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 5\n",
        "counter = 0\n",
        "\n",
        "# Training loop with validation and metrics\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for features, targets in train_loader:\n",
        "        features = features.unsqueeze(2)  # [batch, seq_len, 1]\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, targets in val_loader:\n",
        "            features = features.unsqueeze(2)\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    # Compute metrics\n",
        "    all_preds_binary = [1 if p >= 0.5 else 0 for p in all_preds]\n",
        "    val_acc = accuracy_score(all_targets, all_preds_binary)\n",
        "    val_f1 = f1_score(all_targets, all_preds_binary)\n",
        "    val_roc_auc = roc_auc_score(all_targets, all_preds)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {sum(train_losses)/len(train_losses):.4f} | \"\n",
        "          f\"Val Loss: {sum(val_losses)/len(val_losses):.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | ROC-AUC: {val_roc_auc:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    avg_val_loss = sum(val_losses)/len(val_losses)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), \"best_lstm_model.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n",
        "\n",
        "print(\"LSTM training complete. Best model saved as 'best_lstm_model.pth'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_GyeroUy9zu",
        "outputId": "6af15e7d-faca-48e5-e970-7308637e8d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] Train Loss: 0.6597 | Val Loss: 0.6144 | Val Acc: 0.6623 | Val F1: 0.0000 | ROC-AUC: 0.8035\n",
            "Epoch [2/20] Train Loss: 0.5758 | Val Loss: 0.5225 | Val Acc: 0.7338 | Val F1: 0.4384 | ROC-AUC: 0.8098\n",
            "Epoch [3/20] Train Loss: 0.5230 | Val Loss: 0.5018 | Val Acc: 0.7273 | Val F1: 0.6379 | ROC-AUC: 0.8132\n",
            "Epoch [4/20] Train Loss: 0.5063 | Val Loss: 0.4977 | Val Acc: 0.7273 | Val F1: 0.6182 | ROC-AUC: 0.8124\n",
            "Epoch [5/20] Train Loss: 0.5076 | Val Loss: 0.5178 | Val Acc: 0.7273 | Val F1: 0.6379 | ROC-AUC: 0.8098\n",
            "Epoch [6/20] Train Loss: 0.4985 | Val Loss: 0.5034 | Val Acc: 0.7273 | Val F1: 0.6379 | ROC-AUC: 0.8120\n",
            "Epoch [7/20] Train Loss: 0.5010 | Val Loss: 0.4975 | Val Acc: 0.7208 | Val F1: 0.6195 | ROC-AUC: 0.8120\n",
            "Epoch [8/20] Train Loss: 0.4965 | Val Loss: 0.4978 | Val Acc: 0.7338 | Val F1: 0.6239 | ROC-AUC: 0.8111\n",
            "Epoch [9/20] Train Loss: 0.5024 | Val Loss: 0.4951 | Val Acc: 0.7273 | Val F1: 0.6111 | ROC-AUC: 0.8117\n",
            "Epoch [10/20] Train Loss: 0.4949 | Val Loss: 0.5006 | Val Acc: 0.7403 | Val F1: 0.6429 | ROC-AUC: 0.8107\n",
            "Epoch [11/20] Train Loss: 0.5056 | Val Loss: 0.5099 | Val Acc: 0.7338 | Val F1: 0.6372 | ROC-AUC: 0.8084\n",
            "Epoch [12/20] Train Loss: 0.4982 | Val Loss: 0.4933 | Val Acc: 0.7532 | Val F1: 0.6346 | ROC-AUC: 0.8120\n",
            "Epoch [13/20] Train Loss: 0.4956 | Val Loss: 0.5064 | Val Acc: 0.7338 | Val F1: 0.6435 | ROC-AUC: 0.8117\n",
            "Epoch [14/20] Train Loss: 0.4972 | Val Loss: 0.4921 | Val Acc: 0.7532 | Val F1: 0.6346 | ROC-AUC: 0.8120\n",
            "Epoch [15/20] Train Loss: 0.5008 | Val Loss: 0.4986 | Val Acc: 0.7403 | Val F1: 0.6296 | ROC-AUC: 0.8128\n",
            "Epoch [16/20] Train Loss: 0.4944 | Val Loss: 0.4906 | Val Acc: 0.7727 | Val F1: 0.6535 | ROC-AUC: 0.8135\n",
            "Epoch [17/20] Train Loss: 0.4974 | Val Loss: 0.4997 | Val Acc: 0.7403 | Val F1: 0.6429 | ROC-AUC: 0.8115\n",
            "Epoch [18/20] Train Loss: 0.4908 | Val Loss: 0.4946 | Val Acc: 0.7532 | Val F1: 0.6346 | ROC-AUC: 0.8118\n",
            "Epoch [19/20] Train Loss: 0.4840 | Val Loss: 0.4921 | Val Acc: 0.7597 | Val F1: 0.6408 | ROC-AUC: 0.8124\n",
            "Epoch [20/20] Train Loss: 0.4940 | Val Loss: 0.5095 | Val Acc: 0.7403 | Val F1: 0.6429 | ROC-AUC: 0.8109\n",
            "LSTM training complete. Best model saved as 'best_lstm_model.pth'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Feature Importance from Baselines\n",
        "\n",
        "Finally, we extract and plot feature importances from the trained Decision Tree. In scikit-learn, a tree’s feature_importances_ attribute gives the (normalized) total reduction of the splitting criterion (Gini importance) for each feature. We plot these importances as a bar chart to visualize which features are most influential.\n",
        "\n",
        "Figure: Example bar chart of feature importances (mean decrease in impurity). Higher bars indicate more important features in the model."
      ],
      "metadata": {
        "id": "aPteR9aKzFRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances from the decision tree\n",
        "importances = dt_model.feature_importances_\n",
        "feature_names = ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n",
        "                 'Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
        "\n",
        "# Plot bar chart of importances\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(feature_names, importances, color='skyblue')\n",
        "plt.title(\"Feature Importances (Decision Tree)\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "6er-ld_EzKQo",
        "outputId": "3205a286-6eed-41d9-dc43-05bae4367c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart above shows importances for each feature based on mean decrease in impurity. In practice, you would replace the example data with the actual importances from dt_model trained on the diabetes data. This visualization helps to interpret the model by highlighting which medical measures most strongly influence the prediction."
      ],
      "metadata": {
        "id": "N8BtLyjrzPzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NX9k-nqazRSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Finalize GRU implementation and run initial tests"
      ],
      "metadata": {
        "id": "2R9011K2mqgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this once at the top of your Colab notebook to set up paths and imports used by all blocks.\n",
        "import os, joblib, json, numpy as np\n",
        "ARTIFACT_DIR = \"/content/drive/MyDrive/Diabetes_Project/artifacts\"  # adjust if necessary\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "# try to load preprocessed data if present\n",
        "if 'X_scaled' not in globals() or 'y' not in globals():\n",
        "    X_path = os.path.join(ARTIFACT_DIR, \"X_scaled.npy\")\n",
        "    y_path = os.path.join(ARTIFACT_DIR, \"y.npy\")\n",
        "    if os.path.exists(X_path) and os.path.exists(y_path):\n",
        "        X_scaled = np.load(X_path)\n",
        "        y = np.load(y_path)\n",
        "        print(\"Loaded X_scaled/y from artifacts.\")\n",
        "    else:\n",
        "        raise RuntimeError(\"X_scaled and y not found in session or artifacts. Please run preprocessing first.\")\n"
      ],
      "metadata": {
        "id": "452qrLM3mXrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU implementation + quick test (few epochs)\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Train/test split (use stratify)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# PyTorch dataset/loaders\n",
        "def make_loader(X, y, batch_size=64, shuffle=True):\n",
        "    xt = torch.tensor(X, dtype=torch.float32)\n",
        "    yt = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    ds = TensorDataset(xt, yt)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n",
        "\n",
        "train_loader = make_loader(X_train, y_train, batch_size=64, shuffle=True)\n",
        "test_loader  = make_loader(X_test, y_test, batch_size=128, shuffle=False)\n",
        "\n",
        "# GRU classifier\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        return self.fc(out)\n",
        "\n",
        "# training loop (short initial test)\n",
        "model = GRUClassifier(input_size=1, hidden_size=64, num_layers=2, dropout=0.3).to(DEVICE)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train_one_epoch(loader):\n",
        "    model.train()\n",
        "    losses=[]\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.unsqueeze(2).to(DEVICE)   # [batch, seq_len, 1]\n",
        "        yb = yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    return sum(losses)/len(losses)\n",
        "\n",
        "def eval_on_loader(loader):\n",
        "    model.eval()\n",
        "    probs=[]; trues=[]\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.unsqueeze(2).to(DEVICE)\n",
        "            out = model(xb)\n",
        "            prob = torch.sigmoid(out).cpu().numpy().flatten()\n",
        "            probs.extend(prob)\n",
        "            trues.extend(yb.numpy().flatten())\n",
        "    preds = [1 if p>=0.5 else 0 for p in probs]\n",
        "    return preds, probs, trues\n",
        "\n",
        "# quick test: 10 epochs with early stopping patience=3\n",
        "best_val_auc = -1\n",
        "patience = 3; patience_cnt = 0\n",
        "for epoch in range(1, 11):\n",
        "    tr_loss = train_one_epoch(train_loader)\n",
        "    preds_val, probs_val, y_val = eval_on_loader(test_loader)\n",
        "    val_auc = roc_auc_score(y_val, probs_val)\n",
        "    print(f\"Epoch {epoch:02d} | Train loss {tr_loss:.4f} | Test AUC {val_auc:.4f}\")\n",
        "    # early save\n",
        "    if val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        torch.save(model.state_dict(), os.path.join(ARTIFACT_DIR, \"gru_initial_best.pth\"))\n",
        "        patience_cnt = 0\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        if patience_cnt >= patience:\n",
        "            print(\"Early stopping (initial test).\")\n",
        "            break\n",
        "\n",
        "# final evaluation and report\n",
        "model.load_state_dict(torch.load(os.path.join(ARTIFACT_DIR, \"gru_initial_best.pth\")))\n",
        "preds, probs, trues = eval_on_loader(test_loader)\n",
        "print(\"Classification report (GRU initial test):\")\n",
        "print(classification_report(trues, preds, digits=4))\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(trues, preds))\n",
        "print(\"ROC-AUC:\", roc_auc_score(trues, probs))\n",
        "\n",
        "# Save model (already saved as 'gru_initial_best.pth') and summary\n",
        "joblib.dump({\"best_val_auc\": float(best_val_auc)}, os.path.join(ARTIFACT_DIR,\"gru_initial_summary.joblib\"))\n",
        "print(\"Saved GRU initial artifacts to:\", ARTIFACT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i19BSmxUlV3h",
        "outputId": "29ab2b51-4388-4fe9-d40f-3d1acc68c27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Epoch 01 | Train loss 0.6667 | Test AUC 0.7944\n",
            "Epoch 02 | Train loss 0.6290 | Test AUC 0.7953\n",
            "Epoch 03 | Train loss 0.5820 | Test AUC 0.7980\n",
            "Epoch 04 | Train loss 0.5188 | Test AUC 0.8010\n",
            "Epoch 05 | Train loss 0.4946 | Test AUC 0.7998\n",
            "Epoch 06 | Train loss 0.4975 | Test AUC 0.7999\n",
            "Epoch 07 | Train loss 0.4902 | Test AUC 0.7995\n",
            "Early stopping (initial test).\n",
            "Classification report (GRU initial test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7953    0.8080    0.8016       125\n",
            "         1.0     0.6308    0.6119    0.6212        67\n",
            "\n",
            "    accuracy                         0.7396       192\n",
            "   macro avg     0.7130    0.7100    0.7114       192\n",
            "weighted avg     0.7379    0.7396    0.7386       192\n",
            "\n",
            "Confusion matrix:\n",
            "[[101  24]\n",
            " [ 26  41]]\n",
            "ROC-AUC: 0.800955223880597\n",
            "Saved GRU initial artifacts to: /content/drive/MyDrive/Diabetes_Project/artifacts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Start advanced models: Random Forest & XGBoost"
      ],
      "metadata": {
        "id": "GqxX_FUqmxf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & evaluate RandomForest and XGBoost (baseline/evaluated)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train/test split (same split used above for GRU)\n",
        "# X_train, X_test, y_train, y_test already defined earlier in GRU block; if not, re-run split:\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Random Forest (baseline)\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_probs = rf.predict_proba(X_test)[:,1]\n",
        "rf_preds = rf.predict(X_test)\n",
        "print(\"Random Forest classification report:\\n\", classification_report(y_test, rf_preds, digits=4))\n",
        "print(\"RF Confusion matrix:\\n\", confusion_matrix(y_test, rf_preds))\n",
        "print(\"RF ROC-AUC:\", roc_auc_score(y_test, rf_probs))\n",
        "joblib.dump(rf, os.path.join(ARTIFACT_DIR,\"rf_baseline.joblib\"))\n",
        "\n",
        "# XGBoost (baseline)\n",
        "xgb = XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric=\"logloss\", random_state=42, n_jobs=-1)\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_probs = xgb.predict_proba(X_test)[:,1]\n",
        "xgb_preds = xgb.predict(X_test)\n",
        "print(\"XGBoost classification report:\\n\", classification_report(y_test, xgb_preds, digits=4))\n",
        "print(\"XGB Confusion matrix:\\n\", confusion_matrix(y_test, xgb_preds))\n",
        "print(\"XGB ROC-AUC:\", roc_auc_score(y_test, xgb_probs))\n",
        "joblib.dump(xgb, os.path.join(ARTIFACT_DIR,\"xgb_baseline.joblib\"))\n",
        "\n",
        "# Save a short JSON summary\n",
        "summary = {\n",
        "    \"rf\": {\"roc_auc\": float(roc_auc_score(y_test, rf_probs))},\n",
        "    \"xgb\": {\"roc_auc\": float(roc_auc_score(y_test, xgb_probs))}\n",
        "}\n",
        "with open(os.path.join(ARTIFACT_DIR,\"rf_xgb_baseline_summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"Saved baseline RF/XGB artifacts and summary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6vX9zl1my0x",
        "outputId": "a81d8f96-fb75-4832-c1fe-f8b6ab5cd555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7970    0.8480    0.8217       125\n",
            "           1     0.6780    0.5970    0.6349        67\n",
            "\n",
            "    accuracy                         0.7604       192\n",
            "   macro avg     0.7375    0.7225    0.7283       192\n",
            "weighted avg     0.7555    0.7604    0.7565       192\n",
            "\n",
            "RF Confusion matrix:\n",
            " [[106  19]\n",
            " [ 27  40]]\n",
            "RF ROC-AUC: 0.8180298507462687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:47:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8110    0.8240    0.8175       125\n",
            "           1     0.6615    0.6418    0.6515        67\n",
            "\n",
            "    accuracy                         0.7604       192\n",
            "   macro avg     0.7363    0.7329    0.7345       192\n",
            "weighted avg     0.7589    0.7604    0.7596       192\n",
            "\n",
            "XGB Confusion matrix:\n",
            " [[103  22]\n",
            " [ 24  43]]\n",
            "XGB ROC-AUC: 0.8023880597014925\n",
            "Saved baseline RF/XGB artifacts and summary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Prepare hyperparameter tuning framework (Optuna)\n",
        "\n",
        "The purpose of this cell is to perform hyperparameter tuning on the Random Forest and XGBoost models. You've already established in Cell 7 that these are two of the best-performing baseline models. Hyperparameter tuning is the next logical step in the machine learning workflow to see if you can squeeze even more performance out of them. It's an optimization step that comes after you've chosen your candidate models. Placing this cell after Cell 7 ensures you have the trained models to reference and a clear performance baseline to try and improve upon."
      ],
      "metadata": {
        "id": "0fv4aVlBQs4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "# Optuna setup: study creation functions (you can run short trials)\n",
        "import optuna\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "def run_optuna_rf_quick(X, y, n_trials=20):\n",
        "    def obj(trial):\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 600, step=50),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\",\"log2\", None]),\n",
        "            \"random_state\": RANDOM_SEED\n",
        "        }\n",
        "        skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_SEED)\n",
        "        aucs=[]\n",
        "        for tr, va in skf.split(X, y):\n",
        "            clf = RandomForestClassifier(**params, n_jobs=-1)\n",
        "            clf.fit(X[tr], y[tr])\n",
        "            aucs.append(roc_auc_score(y[va], clf.predict_proba(X[va])[:,1]))\n",
        "        return float(np.mean(aucs))\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
        "                                pruner=optuna.pruners.MedianPruner())\n",
        "    study.optimize(obj, n_trials=n_trials)\n",
        "    joblib.dump(study, os.path.join(ARTIFACT_DIR,\"optuna_study_rf_quick.pkl\"))\n",
        "    print(\"RF quick study done. best AUC:\", study.best_value)\n",
        "    return study\n",
        "\n",
        "def run_optuna_xgb_quick(X, y, n_trials=20):\n",
        "    def obj(trial):\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 600, step=50),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0)\n",
        "        }\n",
        "        skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_SEED)\n",
        "        aucs=[]\n",
        "        for tr, va in skf.split(X, y):\n",
        "            clf = XGBClassifier(**params, use_label_encoder=False, eval_metric=\"logloss\", n_jobs=-1)\n",
        "            clf.fit(X[tr], y[tr], verbose=False)\n",
        "            aucs.append(roc_auc_score(y[va], clf.predict_proba(X[va])[:,1]))\n",
        "        return float(np.mean(aucs))\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
        "                                pruner=optuna.pruners.MedianPruner())\n",
        "    study.optimize(obj, n_trials=n_trials)\n",
        "    joblib.dump(study, os.path.join(ARTIFACT_DIR,\"optuna_study_xgb_quick.pkl\"))\n",
        "    print(\"XGB quick study done. best AUC:\", study.best_value)\n",
        "    return study\n",
        "\n",
        "# Example short runs (adjust n_trials for longer search)\n",
        "study_rf_q = run_optuna_rf_quick(X_scaled, y, n_trials=20)\n",
        "study_xgb_q = run_optuna_xgb_quick(X_scaled, y, n_trials=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxkS0j8_QscW",
        "outputId": "acca8373-a403-4c8f-b88a-d8823abec55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.4/247.4 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.5 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-30 17:47:55,181] A new study created in memory with name: no-name-a94c4f2e-5429-4ea0-a030-1aa38968b0b0\n",
            "[I 2025-08-30 17:47:57,679] Trial 0 finished with value: 0.840865671641791 and parameters: {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:02,455] Trial 1 finished with value: 0.8377910447761194 and parameters: {'n_estimators': 550, 'max_depth': 13, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:05,027] Trial 2 finished with value: 0.8378805970149255 and parameters: {'n_estimators': 200, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:06,300] Trial 3 finished with value: 0.8405074626865671 and parameters: {'n_estimators': 150, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:10,696] Trial 4 finished with value: 0.8343582089552238 and parameters: {'n_estimators': 400, 'max_depth': 3, 'min_samples_split': 13, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:16,604] Trial 5 finished with value: 0.836358208955224 and parameters: {'n_estimators': 500, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:17,719] Trial 6 finished with value: 0.8356417910447762 and parameters: {'n_estimators': 100, 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:19,794] Trial 7 finished with value: 0.8366268656716418 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 16, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:20,873] Trial 8 finished with value: 0.8356119402985075 and parameters: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:23,710] Trial 9 finished with value: 0.8317014925373135 and parameters: {'n_estimators': 250, 'max_depth': 8, 'min_samples_split': 12, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:26,852] Trial 10 finished with value: 0.8395223880597015 and parameters: {'n_estimators': 350, 'max_depth': 15, 'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:29,786] Trial 11 finished with value: 0.8400895522388059 and parameters: {'n_estimators': 300, 'max_depth': 16, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.840865671641791.\n",
            "[I 2025-08-30 17:48:33,450] Trial 12 finished with value: 0.8409552238805971 and parameters: {'n_estimators': 450, 'max_depth': 10, 'min_samples_split': 18, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8409552238805971.\n",
            "[I 2025-08-30 17:48:37,737] Trial 13 finished with value: 0.8412835820895521 and parameters: {'n_estimators': 450, 'max_depth': 11, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.8412835820895521.\n",
            "[I 2025-08-30 17:48:42,315] Trial 14 finished with value: 0.8412835820895521 and parameters: {'n_estimators': 450, 'max_depth': 11, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.8412835820895521.\n",
            "[I 2025-08-30 17:48:47,367] Trial 15 finished with value: 0.84 and parameters: {'n_estimators': 600, 'max_depth': 12, 'min_samples_split': 20, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 13 with value: 0.8412835820895521.\n",
            "[I 2025-08-30 17:48:51,223] Trial 16 finished with value: 0.8405671641791044 and parameters: {'n_estimators': 450, 'max_depth': 14, 'min_samples_split': 18, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.8412835820895521.\n",
            "[I 2025-08-30 17:48:55,661] Trial 17 finished with value: 0.8414029850746269 and parameters: {'n_estimators': 500, 'max_depth': 11, 'min_samples_split': 18, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 17 with value: 0.8414029850746269.\n",
            "[I 2025-08-30 17:49:00,479] Trial 18 finished with value: 0.8425970149253732 and parameters: {'n_estimators': 600, 'max_depth': 17, 'min_samples_split': 17, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 18 with value: 0.8425970149253732.\n",
            "[I 2025-08-30 17:49:06,044] Trial 19 finished with value: 0.8405373134328358 and parameters: {'n_estimators': 600, 'max_depth': 17, 'min_samples_split': 17, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 18 with value: 0.8425970149253732.\n",
            "[I 2025-08-30 17:49:06,052] A new study created in memory with name: no-name-90d3bd2f-0e54-4861-a80b-8278c1a07643\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF quick study done. best AUC: 0.8425970149253732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:06,356] Trial 0 finished with value: 0.8343880597014925 and parameters: {'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.0483437145318464, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182, 'gamma': 0.7799726016810132, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676}. Best is trial 0 with value: 0.8343880597014925.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:07,501] Trial 1 finished with value: 0.8335820895522388 and parameters: {'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.0011152328125494347, 'subsample': 0.9849549260809971, 'colsample_bytree': 0.9162213204002109, 'gamma': 1.0616955533913808, 'reg_alpha': 0.9091248360355031, 'reg_lambda': 0.9170225492671691}. Best is trial 0 with value: 0.8343880597014925.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:07,951] Trial 2 finished with value: 0.8392537313432835 and parameters: {'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.009860942908083906, 'subsample': 0.645614570099021, 'colsample_bytree': 0.8059264473611898, 'gamma': 0.6974693032602092, 'reg_alpha': 1.4607232426760908, 'reg_lambda': 1.8318092164684585}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:08,627] Trial 3 finished with value: 0.8343582089552237 and parameters: {'n_estimators': 350, 'max_depth': 10, 'learning_rate': 0.0028804169778805498, 'subsample': 0.7571172192068059, 'colsample_bytree': 0.7962072844310213, 'gamma': 0.23225206359998862, 'reg_alpha': 3.0377242595071916, 'reg_lambda': 0.8526206184364576}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:08,722] Trial 4 finished with value: 0.8351940298507462 and parameters: {'n_estimators': 100, 'max_depth': 12, 'learning_rate': 0.16670486450654684, 'subsample': 0.9041986740582306, 'colsample_bytree': 0.6523068845866853, 'gamma': 0.48836057003191935, 'reg_alpha': 3.4211651325607844, 'reg_lambda': 2.2007624686980067}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:08,930] Trial 5 finished with value: 0.835044776119403 and parameters: {'n_estimators': 150, 'max_depth': 7, 'learning_rate': 0.0011998556988857205, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085, 'gamma': 3.31261142176991, 'reg_alpha': 1.5585553804470549, 'reg_lambda': 2.600340105889054}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:09,114] Trial 6 finished with value: 0.8383283582089551 and parameters: {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.17023282716867383, 'subsample': 0.8875664116805573, 'colsample_bytree': 0.9697494707820946, 'gamma': 4.474136752138244, 'reg_alpha': 2.9894998940554256, 'reg_lambda': 4.609371175115584}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:09,234] Trial 7 finished with value: 0.8257611940298508 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.0012707770074499693, 'subsample': 0.6626651653816322, 'colsample_bytree': 0.6943386448447411, 'gamma': 1.3567451588694794, 'reg_alpha': 4.143687545759647, 'reg_lambda': 1.7837666334679465}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:09,451] Trial 8 finished with value: 0.8311492537313432 and parameters: {'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.0021099437081941153, 'subsample': 0.9010984903770198, 'colsample_bytree': 0.5372753218398854, 'gamma': 4.9344346830025865, 'reg_alpha': 3.861223846483287, 'reg_lambda': 0.993578407670862}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:09,715] Trial 9 finished with value: 0.8351940298507462 and parameters: {'n_estimators': 100, 'max_depth': 11, 'learning_rate': 0.04231554618260076, 'subsample': 0.8645035840204937, 'colsample_bytree': 0.8856351733429728, 'gamma': 0.3702232586704518, 'reg_alpha': 1.7923286427213632, 'reg_lambda': 0.5793452976256486}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:10,201] Trial 10 finished with value: 0.8364776119402986 and parameters: {'n_estimators': 600, 'max_depth': 7, 'learning_rate': 0.007054333155874016, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.7846562513261506, 'gamma': 2.215398984003511, 'reg_alpha': 4.7988537297270994, 'reg_lambda': 3.3342913754128913}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:10,501] Trial 11 finished with value: 0.8385970149253732 and parameters: {'n_estimators': 450, 'max_depth': 3, 'learning_rate': 0.014912523226681663, 'subsample': 0.6347936794723361, 'colsample_bytree': 0.9812789789973255, 'gamma': 4.374820744386459, 'reg_alpha': 2.3635700332897436, 'reg_lambda': 4.721383939142848}. Best is trial 2 with value: 0.8392537313432835.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:10,846] Trial 12 finished with value: 0.8403880597014926 and parameters: {'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.013504988432973427, 'subsample': 0.642084951685628, 'colsample_bytree': 0.8542877424436464, 'gamma': 3.44768300560321, 'reg_alpha': 2.1347111859662196, 'reg_lambda': 3.4316376453516435}. Best is trial 12 with value: 0.8403880597014926.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:11,261] Trial 13 finished with value: 0.8397014925373134 and parameters: {'n_estimators': 550, 'max_depth': 6, 'learning_rate': 0.009771703790976633, 'subsample': 0.6135695058254392, 'colsample_bytree': 0.8255079015857973, 'gamma': 3.1482476143338163, 'reg_alpha': 1.9093243358979695, 'reg_lambda': 3.435570928215947}. Best is trial 12 with value: 0.8403880597014926.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:11,746] Trial 14 finished with value: 0.838179104477612 and parameters: {'n_estimators': 550, 'max_depth': 5, 'learning_rate': 0.005229521504935193, 'subsample': 0.5511431181724429, 'colsample_bytree': 0.8649174147658507, 'gamma': 2.941685516463366, 'reg_alpha': 2.31057379200564, 'reg_lambda': 3.5996923221275288}. Best is trial 12 with value: 0.8403880597014926.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:12,069] Trial 15 finished with value: 0.8419701492537313 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.021693235895483096, 'subsample': 0.584509269847916, 'colsample_bytree': 0.7261631564388042, 'gamma': 3.4973139810287504, 'reg_alpha': 0.7417172555537601, 'reg_lambda': 3.4764805642426153}. Best is trial 15 with value: 0.8419701492537313.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:12,353] Trial 16 finished with value: 0.841910447761194 and parameters: {'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.02415318044574821, 'subsample': 0.710069141241682, 'colsample_bytree': 0.6867229733335528, 'gamma': 3.798136708991874, 'reg_alpha': 0.02370059826544635, 'reg_lambda': 3.9539985917291287}. Best is trial 15 with value: 0.8419701492537313.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:12,632] Trial 17 finished with value: 0.8428358208955224 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.03187351811440148, 'subsample': 0.6970193560860993, 'colsample_bytree': 0.7361626409651432, 'gamma': 3.9601189045756082, 'reg_alpha': 0.06589607190461733, 'reg_lambda': 3.9488893669262843}. Best is trial 17 with value: 0.8428358208955224.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:12,965] Trial 18 finished with value: 0.8389552238805971 and parameters: {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.07293681501375765, 'subsample': 0.5754408076336635, 'colsample_bytree': 0.7187922036295135, 'gamma': 2.533881284358335, 'reg_alpha': 0.9554799773501501, 'reg_lambda': 2.840956430280529}. Best is trial 17 with value: 0.8428358208955224.\n",
            "/tmp/ipython-input-3803184898.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.2),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:49:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-08-30 17:49:13,362] Trial 19 finished with value: 0.8394029850746269 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.02709127881472873, 'subsample': 0.710883330611204, 'colsample_bytree': 0.757800483824364, 'gamma': 1.7455198366464995, 'reg_alpha': 0.5752534348776998, 'reg_lambda': 4.09713655113397}. Best is trial 17 with value: 0.8428358208955224.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB quick study done. best AUC: 0.8428358208955224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Create feature importance report from baseline models\n",
        "\n",
        "While you performed a basic feature importance analysis on the Decision Tree in Cell 4, this new cell provides a much more comprehensive and robust analysis. It calculates feature importance for the Random Forest and XGBoost models, which are more powerful than a single Decision Tree. It also introduces permutation importance, a model-agnostic method that can be used on any model, including the SVM and deep learning models, which don't have built-in feature importance scores. This deeper analysis and cross-model comparison is a perfect final step to summarize your findings and provide a full picture of which features drive the best-performing models."
      ],
      "metadata": {
        "id": "Y0z7FWIPSA1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance report: RF feature_importances_, XGB feature_importances_, permutation importance for SVM\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "feature_names = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
        "\n",
        "# Ensure baseline models exist: dt (if present), rf, xgb, svm\n",
        "dt = None\n",
        "try:\n",
        "    dt = joblib.load(os.path.join(ARTIFACT_DIR,\"dt_baseline.joblib\"))\n",
        "except:\n",
        "    # train a DT quickly if not present\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "    dt.fit(X_train, y_train)\n",
        "    joblib.dump(dt, os.path.join(ARTIFACT_DIR,\"dt_baseline.joblib\"))\n",
        "\n",
        "rf = joblib.load(os.path.join(ARTIFACT_DIR,\"rf_baseline.joblib\")) if os.path.exists(os.path.join(ARTIFACT_DIR,\"rf_baseline.joblib\")) else rf\n",
        "xgb = joblib.load(os.path.join(ARTIFACT_DIR,\"xgb_baseline.joblib\")) if os.path.exists(os.path.join(ARTIFACT_DIR,\"xgb_baseline.joblib\")) else xgb\n",
        "svm = None\n",
        "try:\n",
        "    svm = joblib.load(os.path.join(ARTIFACT_DIR,\"svm_baseline.joblib\"))\n",
        "except:\n",
        "    pass  # SVM may or may not exist\n",
        "\n",
        "# 1) Decision Tree importances\n",
        "plt.figure(figsize=(8,4))\n",
        "imp_dt = dt.feature_importances_\n",
        "plt.barh(feature_names, imp_dt)\n",
        "plt.title(\"Decision Tree feature importances\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(ARTIFACT_DIR,\"featimp_dt.png\"), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# 2) Random Forest importances\n",
        "plt.figure(figsize=(8,4))\n",
        "imp_rf = rf.feature_importances_\n",
        "plt.barh(feature_names, imp_rf)\n",
        "plt.title(\"Random Forest feature importances\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(ARTIFACT_DIR,\"featimp_rf.png\"), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# 3) XGBoost importances (gain if available)\n",
        "plt.figure(figsize=(8,4))\n",
        "try:\n",
        "    imp_xgb = xgb.feature_importances_\n",
        "    plt.barh(feature_names, imp_xgb)\n",
        "    plt.title(\"XGBoost feature importances\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(ARTIFACT_DIR,\"featimp_xgb.png\"), dpi=300)\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(\"Could not extract xgb.feature_importances_:\", e)\n",
        "\n",
        "# 4) Permutation importance for SVM (or RF as fallback)\n",
        "target_model = svm if svm is not None else rf\n",
        "perm = permutation_importance(target_model, X_test, y_test, n_repeats=30, random_state=42, n_jobs=-1)\n",
        "perm_means = perm.importances_mean\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.barh(feature_names, perm_means)\n",
        "plt.title(\"Permutation importances (model: {})\".format(\"SVM\" if svm is not None else \"RF\"))\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(ARTIFACT_DIR,\"perm_importances.png\"), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# 5) Produce a small JSON summary with ranked features (by XGB if available else RF)\n",
        "def rank_features_by(arr, names):\n",
        "    order = np.argsort(arr)[::-1]\n",
        "    return [{\"feature\": names[i], \"importance\": float(arr[i])} for i in order]\n",
        "\n",
        "ranking = {}\n",
        "if 'imp_xgb' in locals():\n",
        "    ranking['xgb'] = rank_features_by(imp_xgb, feature_names)\n",
        "ranking['rf'] = rank_features_by(imp_rf, feature_names)\n",
        "ranking['dt'] = rank_features_by(imp_dt, feature_names)\n",
        "with open(os.path.join(ARTIFACT_DIR,\"feature_importance_summary.json\"), \"w\") as f:\n",
        "    json.dump(ranking, f, indent=2)\n",
        "\n",
        "print(\"Saved feature importance plots and summary to:\", ARTIFACT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ1_RNXfRhVH",
        "outputId": "cdc5ab29-1fd5-40ef-f3df-865a99fc9070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved feature importance plots and summary to: /content/drive/MyDrive/Diabetes_Project/artifacts\n"
          ]
        }
      ]
    }
  ]
}